{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein2 with mask\",\"KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLGGGGGPPPPQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "token_representations = results[\"representations\"][6]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n",
    "import matplotlib.pyplot as plt\n",
    "for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
    "    plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "    plt.title(seq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def filter_fasta(input_file, output_file, min_length=10, tax_filter=None, keyword=None, max_len=2000, max_writes=100000):\n",
    "    num_records = 0\n",
    "    i = 0\n",
    "    write_every = max(1, 69000000 // max_writes)\n",
    "\n",
    "    with open(output_file, \"w\") as out_handle:\n",
    "        for record in tqdm(SeqIO.parse(input_file, \"fasta\")):\n",
    "            i += 1\n",
    "            desc = record.description\n",
    "\n",
    "            # Apply filters\n",
    "            if len(record.seq) < min_length:\n",
    "                continue\n",
    "            if len(record.seq) > max_len:\n",
    "                continue\n",
    "            if tax_filter and f\"Tax={tax_filter}\" not in desc:\n",
    "                continue\n",
    "            if keyword and keyword.lower() not in desc.lower():\n",
    "                continue\n",
    "\n",
    "            # Throttle writes to limit total\n",
    "            if i % write_every != 0:\n",
    "                continue\n",
    "\n",
    "            SeqIO.write(record, out_handle, \"fasta\")\n",
    "            print(\"writing\", record.id)\n",
    "            num_records += 1\n",
    "\n",
    "            if num_records >= max_writes:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter large file to smaller one\n",
    "# filter_fasta(\"../data/uniref50.fasta\", \"filtered_len_less_100.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a67ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_batches(file_path, batch_size=1):\n",
    "    \"\"\"\n",
    "    Generator that yields batches of records from a FASTA file.\n",
    "    Args:\n",
    "        file_path (_type_): _description_\n",
    "        batch_size (int, optional): _description_. Defaults to 1.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as handle:\n",
    "        record_iter = SeqIO.parse(handle, \"fasta\")\n",
    "        while True:\n",
    "            batch = list(next(record_iter) for _ in range(batch_size))\n",
    "            if not batch:\n",
    "                break\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_1 = fasta_batches(\"../data/uniref50.fasta\", 1)\n",
    "batch_1 = fasta_batches(\"filtered_len_less_100.fasta\", batch_size=10)\n",
    "data_1 = []\n",
    "for batch in batch_1:\n",
    "    print(batch)\n",
    "    for record in batch:\n",
    "        print(record)\n",
    "        print(f\"ID: {record.id}\")\n",
    "        print(f\"Description: {record.description}\")\n",
    "        print(f\"Sequence: {record.seq}\\n\")\n",
    "        data_1.append((record.id, str(record.seq)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91282ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels, batch_strs, batch_tokens = batch_converter(data_1)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "token_representations = results[\"representations\"][6]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, tokens_len in enumerate(batch_lens):\n",
    "    sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "\n",
    "# Look at the unsupervised self-attention map contact predictions\n",
    "import matplotlib.pyplot as plt\n",
    "for (_, seq), tokens_len, attention_contacts in zip(data_1, batch_lens, results[\"contacts\"]):\n",
    "    plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "    plt.title(seq)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcd1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e6b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ac945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
