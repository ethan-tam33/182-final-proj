{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# takes FASTA files and generates dataframe and csv with descriptions and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba56cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"seq_df_first_10000_less_1000.csv\", index_col=0).dropna()\n",
    "cols = list(df.columns)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1692606",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_feature = \"Molecular Weight\"\n",
    "assert sort_feature in cols, \"Feature not in df.columns\"\n",
    "#df = df.sort_values(by=sort_feature, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02feb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed17a5e",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b88739",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"GRAVY\"], bins=200)\n",
    "plt.xlabel(\"GRAVY\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of GRAVY values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609adcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"Aromaticity\"], bins=50)\n",
    "plt.xlabel(\"Aromaticity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Aromaticity values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"Molecular Weight\"], bins=100)\n",
    "plt.xlabel(\"MW\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of MW values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5259ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"Charge at pH:7.0\"], bins=200)\n",
    "plt.xlabel(\"Aromaticity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Charge at pH:7.0 values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42832b20",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ec8d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 640, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (12): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (13): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (14): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (15): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (16): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (17): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (18): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (19): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (20): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (21): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (22): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (23): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (24): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (25): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (26): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (27): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (28): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (29): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ESM-2 model\n",
    "#model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22003be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, model):    \n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "    results[\"token_representations\"] = token_representations\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0).numpy())\n",
    "    results['sequence_representations'] = torch.tensor(np.array(sequence_representations)) # first to np for speed-up\n",
    "    # Look at the unsupervised self-attention map contact predictions\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n",
    "    #     plt.matshow(attention_contacts[: tokens_len, : tokens_len])\n",
    "    #     plt.title(seq[:10] + \"...\")\n",
    "    #     plt.colorbar()\n",
    "    #     plt.show()    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_gravy = df.iloc[:10]\n",
    "low_gravy = df.iloc[-10:]\n",
    "\n",
    "top_gravy_X = [(top_gravy.iloc[i][\"Name\"], top_gravy.iloc[i][\"Sequence\"]) for i in range(len(top_gravy))]\n",
    "low_gravy_X = [(low_gravy.iloc[i][\"Name\"], low_gravy.iloc[i][\"Sequence\"]) for i in range(len(low_gravy))]\n",
    "top_gravy_X, low_gravy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff420581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a while...\n",
    "top_gravy_results = get_features(top_gravy_X, model)\n",
    "low_gravy_results = get_features(low_gravy_X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e73906",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_high = dict(top_gravy_results)\n",
    "print(res_high.keys())\n",
    "res_low = dict(low_gravy_results)\n",
    "print(res_low.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_high = res_high['sequence_representations'] # top feature representations\n",
    "plt.imshow(sr_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dae341",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_low = res_low['sequence_representations'] # top feature representations\n",
    "plt.imshow(sr_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cbe51",
   "metadata": {},
   "source": [
    "### Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1290929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d722c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_high = sr_high[:]\n",
    "sr_high_std = sr_high - sr_high.mean(axis=0)\n",
    "sr_high_std /= sr_high_std.std(axis=0)\n",
    "\n",
    "sr_low = sr_low[:]\n",
    "sr_low_std = sr_low - sr_low.mean(axis=0)\n",
    "sr_low_std /= sr_low_std.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92868c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "sr_high_pca = pca.fit_transform(sr_high_std)\n",
    "sr_low_pca = pca.fit_transform(sr_low_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadf23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sr_high_pca[:, 0], sr_high_pca[:, 1])\n",
    "plt.scatter(sr_low_pca[:, 0], sr_low_pca[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax  = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# 3-D scatter for the two classes\n",
    "ax.scatter(\n",
    "    sr_high_pca[:, 0], sr_high_pca[:, 1], sr_high_pca[:, 2],\n",
    "    s=20, alpha=0.7, label=\"high SR\", marker=\"o\"\n",
    ")\n",
    "ax.scatter(\n",
    "    sr_low_pca[:, 0], sr_low_pca[:, 1], sr_low_pca[:, 2],\n",
    "    s=20, alpha=0.7, label=\"low SR\",  marker=\"^\"\n",
    ")\n",
    "\n",
    "# Axis labels (use real explained-variance % if you have pca.explained_variance_ratio_)\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_title(\"3-D PCA projection\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53931fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    n_components     = 3,      # 2-D for plotting (use 3 for 3-D)\n",
    "    perplexity       = 1,     # “effective #neighbors” (5–50 typical)\n",
    "    learning_rate    = \"auto\", # or a value ~ n_samples / 12\n",
    "    init             = \"pca\",  # pca|random; pca gives stabler layouts\n",
    "    n_iter           = 2000,   # 250–1000 is common; more for cleaner structure\n",
    "    metric           = \"euclidean\",\n",
    "    random_state     = 0,\n",
    "    verbose          = 1\n",
    ")\n",
    "Z = tsne.fit_transform(sr_high_std)    \n",
    "Z_2 = tsne.fit_transform(sr_low_std) # shape (n_samples, 2)\n",
    "\n",
    "# 4) Visualise  ---------------------------------------------------------\n",
    "plt.scatter(Z[:,0], Z[:,1], s=10, alpha=0.7)\n",
    "plt.scatter(Z_2[:,0], Z_2[:,1], s=10, alpha=0.7)\n",
    "plt.title(\"t-SNE projection\"); plt.xlabel(\"t-SNE-1\"); plt.ylabel(\"t-SNE-2\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax  = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# 3-D scatter for the two classes\n",
    "ax.scatter(\n",
    "    Z[:, 0], Z[:, 1], Z[:, 2],\n",
    "    s=20, alpha=0.7, label=\"high SR\", marker=\"o\"\n",
    ")\n",
    "ax.scatter(\n",
    "    Z_2[:, 0], Z_2[:, 1], Z_2[:, 2],\n",
    "    s=20, alpha=0.7, label=\"low SR\",  marker=\"^\"\n",
    ")\n",
    "\n",
    "# Axis labels (use real explained-variance % if you have pca.explained_variance_ratio_)\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_title(\"3-D PCA projection\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0a30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample, calculate representation\n",
    "out_cols = [\"Name\", \"Sequence\", \"Token Representations\", \"Sequence Representations\", \"Contacts\"]\n",
    "outputs = [] # stores rows with name, sequence as strings and representations and contacts urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b53062d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5892ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [43:29,  4.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7d131b2daf5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         ]\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# generate paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtoken_representations_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"token_representations/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-94fc56e58e1f>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract per-residue representations (on CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_contacts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtoken_representations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"representations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_representations\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/esmfold/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/182-proj/182-final-proj/esm/esm/model/esm2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself_attn_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mneed_head_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_head_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepr_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/esmfold/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/182-proj/182-final-proj/esm/esm/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mneed_head_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_head_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/esmfold/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/182-proj/182-final-proj/esm/esm/multihead_attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/esmfold/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/esmfold/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# unbatched\n",
    "output_path = \"sample_output/\"\n",
    "\n",
    "for row in tqdm.tqdm(df[[\"Name\", \"Sequence\"]].itertuples()):\n",
    "    batch = [\n",
    "        (row.Name, row.Sequence)\n",
    "        ]\n",
    "    res = get_features(batch, model)\n",
    "    # generate paths\n",
    "    token_representations_path = output_path + \"token_representations/\" + str(row.Name) + \".pt\"\n",
    "    sequence_representations_path = output_path + \"sequence_representations/\" + str(row.Name) + \".pt\"\n",
    "    contacts_path = output_path + \"contacts/\" + str(row.Name) + \".pt\"\n",
    "    # save tensors as .pt\n",
    "    torch.save(res['token_representations'][0], token_representations_path) # uses index 0 due to being unbatched\n",
    "    torch.save(res['sequence_representations'][0], sequence_representations_path)\n",
    "    torch.save(res['contacts'][0], contacts_path)\n",
    "    # save urls to list for csv\n",
    "    outputs.append([row.Name, row.Sequence, token_representations_path, sequence_representations_path, contacts_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6b76bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(outputs[1:], columns=out_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86a41757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Token Representations</th>\n",
       "      <th>Sequence Representations</th>\n",
       "      <th>Contacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UniRef50_A0AA42W546</td>\n",
       "      <td>TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0AA42W546.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UniRef50_A0A6G3PS79</td>\n",
       "      <td>PAPGGGTRLLAYATPAAGPLAPDRLREALRTRLPDYLVPAAVIPVD...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A6G3PS79.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniRef50_A0A351Y3E2</td>\n",
       "      <td>MQFSNDDITRSSREAFATTKAWTLSAGTGTKTVYANFDTDNNTATI...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A351Y3E2.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UniRef50_A0AA42W546</td>\n",
       "      <td>TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0AA42W546.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UniRef50_A0AA42W546</td>\n",
       "      <td>TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0AA42W546.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>UniRef50_H2J5P2</td>\n",
       "      <td>MKKSILNTMAIILVFLGILAISGNFFLQIQKEQKSLVILKKSIEVI...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_H...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_H2J5P2.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>UniRef50_A0A7W4G808</td>\n",
       "      <td>MMKNKVQLITYADRITGQDINALTTLLNGPLHGVFGGVHLLPFYNP...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A7W4G808.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>UniRef50_A0A2V8LRK7</td>\n",
       "      <td>MFWLALELTGRRVVAAIAGTLFLVHPIQTQAVTYITQRFESQAALF...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A2V8LRK7.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>UniRef50_A0A950WP72</td>\n",
       "      <td>MPGSVLTRSFDLARSGANTSETILTPDAVRTRGVKQVFVLQTPDDP...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A950WP72.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>UniRef50_A0A2A4NCA6</td>\n",
       "      <td>MRKFFLLGLITLVAISVFAQDTTVTFKLFLGTGEYSPKDDWHGILR...</td>\n",
       "      <td>sample_output/token_representations/UniRef50_A...</td>\n",
       "      <td>sample_output/sequence_representations/UniRef5...</td>\n",
       "      <td>sample_output/contacts/UniRef50_A0A2A4NCA6.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                           Sequence  \\\n",
       "0    UniRef50_A0AA42W546  TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...   \n",
       "1    UniRef50_A0A6G3PS79  PAPGGGTRLLAYATPAAGPLAPDRLREALRTRLPDYLVPAAVIPVD...   \n",
       "2    UniRef50_A0A351Y3E2  MQFSNDDITRSSREAFATTKAWTLSAGTGTKTVYANFDTDNNTATI...   \n",
       "3    UniRef50_A0AA42W546  TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...   \n",
       "4    UniRef50_A0AA42W546  TGQLQFNANGNFTYTPAPGEEGTVTFKYSITDGDGDVSEATVTITL...   \n",
       "..                   ...                                                ...   \n",
       "575      UniRef50_H2J5P2  MKKSILNTMAIILVFLGILAISGNFFLQIQKEQKSLVILKKSIEVI...   \n",
       "576  UniRef50_A0A7W4G808  MMKNKVQLITYADRITGQDINALTTLLNGPLHGVFGGVHLLPFYNP...   \n",
       "577  UniRef50_A0A2V8LRK7  MFWLALELTGRRVVAAIAGTLFLVHPIQTQAVTYITQRFESQAALF...   \n",
       "578  UniRef50_A0A950WP72  MPGSVLTRSFDLARSGANTSETILTPDAVRTRGVKQVFVLQTPDDP...   \n",
       "579  UniRef50_A0A2A4NCA6  MRKFFLLGLITLVAISVFAQDTTVTFKLFLGTGEYSPKDDWHGILR...   \n",
       "\n",
       "                                 Token Representations  \\\n",
       "0    sample_output/token_representations/UniRef50_A...   \n",
       "1    sample_output/token_representations/UniRef50_A...   \n",
       "2    sample_output/token_representations/UniRef50_A...   \n",
       "3    sample_output/token_representations/UniRef50_A...   \n",
       "4    sample_output/token_representations/UniRef50_A...   \n",
       "..                                                 ...   \n",
       "575  sample_output/token_representations/UniRef50_H...   \n",
       "576  sample_output/token_representations/UniRef50_A...   \n",
       "577  sample_output/token_representations/UniRef50_A...   \n",
       "578  sample_output/token_representations/UniRef50_A...   \n",
       "579  sample_output/token_representations/UniRef50_A...   \n",
       "\n",
       "                              Sequence Representations  \\\n",
       "0    sample_output/sequence_representations/UniRef5...   \n",
       "1    sample_output/sequence_representations/UniRef5...   \n",
       "2    sample_output/sequence_representations/UniRef5...   \n",
       "3    sample_output/sequence_representations/UniRef5...   \n",
       "4    sample_output/sequence_representations/UniRef5...   \n",
       "..                                                 ...   \n",
       "575  sample_output/sequence_representations/UniRef5...   \n",
       "576  sample_output/sequence_representations/UniRef5...   \n",
       "577  sample_output/sequence_representations/UniRef5...   \n",
       "578  sample_output/sequence_representations/UniRef5...   \n",
       "579  sample_output/sequence_representations/UniRef5...   \n",
       "\n",
       "                                          Contacts  \n",
       "0    sample_output/contacts/UniRef50_A0AA42W546.pt  \n",
       "1    sample_output/contacts/UniRef50_A0A6G3PS79.pt  \n",
       "2    sample_output/contacts/UniRef50_A0A351Y3E2.pt  \n",
       "3    sample_output/contacts/UniRef50_A0AA42W546.pt  \n",
       "4    sample_output/contacts/UniRef50_A0AA42W546.pt  \n",
       "..                                             ...  \n",
       "575      sample_output/contacts/UniRef50_H2J5P2.pt  \n",
       "576  sample_output/contacts/UniRef50_A0A7W4G808.pt  \n",
       "577  sample_output/contacts/UniRef50_A0A2V8LRK7.pt  \n",
       "578  sample_output/contacts/UniRef50_A0A950WP72.pt  \n",
       "579  sample_output/contacts/UniRef50_A0A2A4NCA6.pt  \n",
       "\n",
       "[580 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d879a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save csv\n",
    "out_df.to_csv(\"model_outputs_1/UniRef50_first_580.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c64f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
